BYTEZ ALL DOCS PASTED:

# Get started

> Install our API libraries and run inference in seconds

<AccordionGroup>
  <Accordion icon="1" title="Install sdk">
    You can use the API with any programming language that can make HTTP requests,
    but we recommend using one of our official SDKs for ease of use and convenience.

    We have libraries for Python, JavaScript, and Julia.
    Using Python 3.9+, JavaScript, or Julia, install the appropriate package:

    <CodeGroup>
      ```bash javascript theme={null}
        // use your favorite package manager
        npm i bytez.js
        yarn add bytez.js
      ```

      ```bash python theme={null}
        pip install bytez
      ```
    </CodeGroup>
  </Accordion>

  <Accordion icon="2" title="Auth">
    Bytez allows you to use open-source and closed-source models with a single API key.

    ## Use open-source models

    1. Copy your key from the [API Dashboard](https://bytez.com/api).
    2. Use your Bytez Key in requests

    <CodeGroup>
      ```javascript javascript theme={null}
      import Bytez from "bytez.js";

      const sdk = new Bytez("BYTEZ_KEY");
      ```

      ```python python theme={null}
      from bytez import Bytez

      sdk = Bytez("BYTEZ_KEY")
      ```

      ```bash http theme={null}
      # add an Authorization header, with value "Key {BYTEZ_KEY}"
      curl -X GET "https://api.bytez.com/models/v2/some-endpoint" \
        -H "Authorization: BYTEZ_KEY"
      ```
    </CodeGroup>

    You're now set to use open source models on Bytez!

    ## Use closed-source models

    To use a closed-source model, you'll need an account with the model provider. For example, if you want to use `OpenAI` models, you'll need an OpenAI key. We call your closed source key a "provider key"

    1. Copy your key from the [API Dashboard](https://bytez.com/api).
    2. Use your Bytez Key AND closed-source provider key in requests.

    <CodeGroup>
      ```bash http theme={null}
      # add a "provider-key" header, and set its value to "{KEY}"
      curl -X GET "https://api.bytez.com/models/v2/some-endpoint" \
        -H "Authorization: BYTEZ_KEY" \
        -H "provider-key: {your-key}"
      ```
    </CodeGroup>

    <Card title="Bytez API Key Security & Usage" icon="lock">
      We securely route your requests as a pass-through service. Your API keys are never stored or logged by Bytez; they are only used to authenticate directly with the model provider.

      **Recommendation:** Use a dedicated API key for Bytez for maximum security and traceability.

      * **Billing:** No extra Bytez fees for closed-source models; you're billed directly by the provider based on usage associated with your key.
      * **Integration:** Seamlessly use the same input format for all models (open and closed-source).
    </Card>

    If you need help with any of this, please DM us in [Discord](https://discord.com/invite/Z723PfCFWf)
    or submit an issue on [GitHub](https://github.com/Bytez-com/docs/issues). We're happy to help.
  </Accordion>

  <Accordion icon="3" title="Run a model">
    Running is a model is easy. Just select the model and pass it an input

    <CodeGroup>
      ```javascript javascript theme={null}
      import Bytez from "bytez.js";

      const sdk = new Bytez("BYTEZ_KEY");
      const modelId = "openai-community/gpt-2"
      const model = sdk.model(model_id)

      const { error, output } = await model.run("Once upon a time")

      console.log({ error, output });
      ```

      ```python python theme={null}
      from bytez import Bytez

      sdk = Bytez("BYTEZ_KEY")

      model = sdk.model("openai-community/gpt-2")

      result = model.run("Once upon a time")

      print(result.output)
      ```

      ```bash http theme={null}
      curl -X POST "https://api.bytez.com/models/v2/openai-community/gpt-2" \
      -H "Authorization: BYTEZ_KEY" \
      -H "Content-Type: application/json" \
      --data '{ "text": "Once upon a time" }'
      ```
    </CodeGroup>

    <Card arrow horizontal icon="globe" href="/http-reference/model/run">
      Read more about our schema by visiting our HTTP reference.
    </Card>
  </Accordion>

  <Accordion icon="4" title="List tasks and models">
    You can list all the tasks, models, and running models using the API.

    ### Tasks

    A `task` defines a specific function a model performs (e.g., object-detection). Multiple models might be available for the same task. To list all tasks supported by Bytez, run the following command:

    <CodeGroup>
      ```javascript javascript theme={null}
      import Bytez from "bytez.js";

      const sdk = new Bytez("BYTEZ_KEY");

      const { error, output } = await sdk.list.tasks()

      console.log({ error, output });
      ```

      ```python python theme={null}
      from bytez import Bytez

      sdk = Bytez("BYTEZ_KEY")

      result = sdk.list.tasks()

      print(result.output)
      ```

      ```bash http theme={null}
      curl -X GET "https://api.bytez.com/models/v2/list/tasks" \
        -H "Authorization: BYTEZ_KEY"
      ```
    </CodeGroup>

    ### Models

    A `model` refers to a software function with unique identifier. `Models` execute `tasks`. For example, the model `google/vit-base-patch16-224` executes `image-classification`. To list all open-source models supported by Bytez, run the following command:

    <CodeGroup>
      ```javascript javascript theme={null}
      import Bytez from "bytez.js";

      const sdk = new Bytez("BYTEZ_KEY");

      const { error, output } = await sdk.list.models()

      console.log({ error, output });
      ```

      ```python python theme={null}
      from bytez import Bytez

      sdk = Bytez("BYTEZ_KEY")

      result = sdk.list.models()

      print(result.output)
      ```

      ```bash http theme={null}
      curl -X GET "https://api.bytez.com/models/v2/list/models" \
        -H "Authorization: BYTEZ_KEY"
      ```
    </CodeGroup>
  </Accordion>
</AccordionGroup>


---

> To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.bytez.com/llms.txt

Open vs Closed models
We handle open & closed models differently

Our API simplifies working with a wide variety of AI models, including both popular closed-source options and flexible open-source alternatives. Although how we handle requests differs behind the scenes depending on the model type, you benefit significantly from our Unified Model Protocol.
This protocol means you can use the same input format to interact with any model on our platform—whether it’s open or closed-source. It makes experimenting and switching between models much easier, almost like swapping Lego bricks in your project. This consistency frees you up to focus purely on building your application logic, rather than managing different provider interfaces.
Now, let’s dive into the specific ways we handle requests under the hood for open vs. closed models to provide this seamless experience.
Closed-Source Models (e.g., OpenAI, Anthropic, Gemini)

Think of us as a smart, multi-lingual translator and secure messenger when you use closed-source models. Our Unified Model Protocol means you use one consistent format for your requests and receive responses in one consistent format, regardless of the underlying provider.
The Process:
1
You Send Request

Your app sends an API request using our standardized input format
2
We Translate Input

We automatically translate your request into the specific format required by the chosen model provider (e.g., OpenAI, Google Gemini)
3
Forward Request

We securely pass your request to the model provider’s API, using your API key, so the provider knows it’s from you
4
Provider Computes

The provider runs inference on their servers
5
We Translate Output

We receive the provider’s raw response and translate to standardized JSON
6
You Receive Response

Your app gets inference results back in standardized JSON
Key Takeaway: For closed-source models, we act as a router and standardization layer. You interact with a single, unified protocol, making it easy to switch between models providers or use multiple providers without changing your code structure. The inference itself happens on the provider’s infrastructure.
Open‑Source Models –  Serverless GPU Inference

When you run an open‑source model, Bytez handles all the heavy lifting for you.
When you make a request to our API, this is what we do:
1
Start a model container

If the model is not immediately available, we spin up a model container on our infrastructure.
2
Route requests

When the model is ready for inference, we route your request to the first available instance.
3
Scale according to load

As requests come in, we scale automatically, ensuring that your scaling demands can be met regardless of the model you choose.
All you need to worry about is specifying a model and making requests to the API, we take care of the rest!
Our goal with open source models is to make them as easy and affordable to use closed source models.

Billing & Credits
How billing works for open and closed models

Bytez uses a credit-based system. Credits are consumed when you run models, and how they’re consumed depends on whether you’re using closed-source or open-source models.
​
Plans
Free
$0 / month - Get $1 in free credits
Run open models up to 7B parameters
Access all closed model providers
1 concurrent request (open models)
10 requests/second (closed models)
Credits refresh every 4 weeks
Pay-as-you-go
$3 / month - Get $5 in credits
Run open models up to 120B parameters
Access all closed model providers
Rate limits scale with credits purchased
Unlimited closed model requests
Add credits anytime
​
How Credits Work
Credits are a unified currency across all models on Bytez. When you run a model, credits are deducted from your balance based on usage.
Model Type	How Credits Are Consumed
Closed models	Based on provider pricing (per token, per image, per video, etc.)
Open models	Per second of inference
Your credits purchased in the last 4 weeks determine two things:
Which open models you can access - Larger open models require more credits purchased to unlock
Your rate limits - More credits purchased unlocks more concurrent requests
Adding credits immediately unlocks higher tiers. You don’t need to wait for the next billing cycle.
​
Credit Unlock Thresholds
Credits Purchased (last 4 weeks)	Open Model Access	Concurrent Requests (7B)
$0 (Free)	Up to 7B	1
$3+	Up to 7B	4
$10+	Up to 35B	4
$25+	Up to 70B	10
$50+	Up to 120B	20
$100+	Up to 120B	40
$500+	Up to 120B	200
$1,000+	Up to 120B	400
Credits expire 4 weeks after purchase. Use them or lose them!
​
Closed Model Billing
For closed-source models (OpenAI, Anthropic, Google, Mistral, Cohere), we pass through the provider’s pricing plus a small platform fee.
Your cost = Provider price + 2% platform fee
Providers charge differently depending on the model and modality - per token for text, per image for image generation, per second for video, etc. We pass through whatever the provider charges.
Example: If OpenAI charges $0.000001 per M tokens, you pay $0.00000102 per M tokens.
Why the 2% fee?

​
What’s included
Pass-through pricing - Pay only for what the provider charges
No minimum - No monthly minimums or commitments
Real-time pricing - We pass through provider rates as they change
​
Open Model Billing
Open-source models run on our serverless GPU infrastructure. You’re billed per second of inference time - no cold start fees, no idle charges.
Your cost = Inference time (seconds) x Rate for model size
​
Pricing by Model Size
Bigger models use more VRAM, so they cost more per second:
Model Size	Per Second	Per Hour
7B	$0.000072	~$0.26
15B	$0.000108	~$0.39
35B	$0.000144	~$0.52
70B	$0.000216	~$0.78
120B	$0.00036	~$1.30
How we calculate pricing

​
What’s included
Per-second billing - Billed in 1-second increments
No cold start fees - You don’t pay while the model loads
No idle charges - You don’t pay when not running inference
No reserved instances - No commitments, no minimums
​
Auto-Reload
Auto-reload automatically tops up your credit balance when it runs low, so your API calls never fail unexpectedly.
​
How it works
Setting	Default	Description
Threshold	$3	Reload triggers when balance drops below this
Reload amount	$10	Amount added to your balance
Monthly max	$100	Maximum auto-reload spend per month
1
Balance drops below threshold

When your credit balance falls below $3 (default), auto-reload activates
2
Card is charged

Your saved payment method is charged $10 (default reload amount)
3
Credits are added

$10 in credits is immediately added to your balance
4
Monthly cap enforced

Auto-reload stops if you’ve hit your monthly maximum ($100 default)
​
If Auto-Reload is Disabled
When auto-reload is off and your credits run out, you may get an API response like this:
{
  "status": 402,
  "error": "Payment Required",
  "message": "Insufficient credits. Please add credits to continue."
}
If you’re running production workloads, we recommend enabling auto-reload to prevent unexpected failures.
​
Configuring Auto-Reload
You can enable, disable, or adjust auto-reload settings in your API Dashboard.
​
Auto-Scaling (Open Models)
By default, if you exceed your open model rate limits, requests are rejected with a rate-limit error.
If you want your rate limits to automatically scale with your traffic in production, add autoScale: true to your request:
const response = await fetch('https://api.bytez.com/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': API_KEY,
    'Content-Type': 'application/json'
  },
  body: JSON.stringify({
    model: 'meta-llama/Llama-3-70b',
    messages: [...],
    autoScale: true
  })
});
When enabled, the system auto-purchases extra credits required to keep auto-scaling. You can control your Max Monthly Spend in your API Dashboard to cap costs. This way you can auto-scale and control your budget.
For closed models, you get unlimited rate limits on a pay-as-you-go basis - no auto-scaling needed.
​
Billing Cycle
Free Plan

Pay-as-you-go Plan

​
Adding Credits Mid-Cycle
You can add credits at any time. When you do:
Immediate access - Higher model tiers and rate limits unlock instantly
No proration - You get the full credit amount immediately
Credits stack - Purchased credits add to your existing balance

Open AI Completions
Chat Completions
Use OpenAI-compatible endpoints for chat and text completions via OpenAI clients, supporting streaming and custom parameters.

Provides chat completions for all open source models that are chat, audio-text-to-text, image-text-to-text, video-text-to-text, it also supports chat completions models from the closed source providers, openai, anthropic, mistral, cohere, and google.
To specify a provider, prefix the model with the provider, e.g. gpt-4 should be passed in as openai/gpt4
We provide access to models from openai, mistral, and google.
You will need to supply a header provider-key in order to make requests to anthropic, and cohere models.
e.g. If you are trying to run anthropic/claude-sonnet-4-5, provider-key will be an Anthropic key.
For unlimited rate limits you will need to supply a header provider-key.
NOTE: Logprobs are supported for all models!
Basic usage (Open Source)


javascript

python

http
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "BYTEZ_KEY",
  baseURL: "https://api.bytez.com/models/v2/openai/v1"
});

const messages = [
  { role: "system", content: "You are a friendly chatbot" },
  { role: "assistant", content: "Hello, I'm a friendly bot" },
  { role: "user", content: "Hello bot, what is the capital of England?" }
];

const response = await client.chat.completions.create({
  model: "Qwen/Qwen3-4B",
  messages,
  max_tokens: 150,
  temperature: 0.7
});

console.log(response);
Streaming (Open Source)


javascript

python

http
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "BYTEZ_KEY",
  baseURL: "https://api.bytez.com/models/v2/openai/v1"
});

const messages = [
  { role: "system", content: "You are a friendly chatbot" },
  { role: "assistant", content: "Hello, I'm a friendly bot" },
  { role: "user", content: "Hello bot, what is the capital of England?" }
];

const stream = await client.chat.completions.create({
  model: "Qwen/Qwen3-4B",
  messages,
  max_tokens: 150,
  temperature: 0.7,
  stream: true
});

let text = '';
for await (const event of stream) {
  if (event.choices[0].finish_reason) {
    break;
  }

  const content = event.choices[0].delta.content;
  text += content;
  console.log(content);
}

console.log({ text });
Basic usage (Closed Source)


javascript

python

http
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "BYTEZ_KEY",
  baseURL: "https://api.bytez.com/models/v2/openai/v1"
});

const messages = [
  { role: "system", content: "You are a friendly chatbot" },
  { role: "assistant", content: "Hello, I'm a friendly bot" },
  { role: "user", content: "Hello bot, what is the capital of England?" }
];

const response = await client.chat.completions.create({
  model: "openai/gpt-4",
  messages,
  max_tokens: 150,
  temperature: 0.7
});

console.log(response);
Streaming (Closed Source)


javascript

python

http
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "BYTEZ_KEY",
  baseURL: "https://api.bytez.com/models/v2/openai/v1"
});

const messages = [
  { role: "system", content: "You are a friendly chatbot" },
  { role: "assistant", content: "Hello, I'm a friendly bot" },
  { role: "user", content: "Hello bot, what is the capital of England?" }
];

const stream = await client.chat.completions.create({
  model: "openai/gpt-4",
  messages,
  max_tokens: 150,
  temperature: 0.7,
  stream: true
});

let text = '';
for await (const event of stream) {
  if (event.choices[0].finish_reason) {
    break;
  }

  const content = event.choices[0].delta.content;
  text += content;
  console.log(content);
}

console.log({ text });

Open AI Completions
Completions
Use OpenAI-compatible endpoints for chat and text completions via OpenAI clients, supporting streaming and custom parameters.

Provides completions for all open source models that are text-generation, it also supports completions models from the closed source providers, openai, anthropic, mistral, cohere, and google.
To specify a provider, prefix the model with the provider, e.g. davinci-002 should be passed in as openai/davinci-002
We provide access to models from openai, mistral, and google.
You will need to supply a header provider-key in order to make requests to anthropic, and cohere models.
For unlimited rate limits you will need to supply a header provider-key.
e.g. If you are trying to run openai/davinci-002 with unlimited rate limits, provider-key will be an Open AI key.
If it were an anthropic model it would be an Anthropic key.
NOTE: Logprobs are supported for all models!
Basic usage (Open Source)


javascript

python

http
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "BYTEZ_KEY",
  baseURL: "https://api.bytez.com/models/v2/openai/v1"
});

const response = await client.completions.create({
  model: "openai-community/gpt2",
  prompt: "Write a short poem about AI",
  temperature: 0.7,
  max_tokens: 150
});

console.log(response);
Streaming (Open Source)


javascript

python

http
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "BYTEZ_KEY",
  baseURL: "https://api.bytez.com/models/v2/openai/v1"
});

const stream = await client.completions.create({
  model: "openai-community/gpt2",
  prompt: "Write a short poem about AI",
  max_tokens: 150,
  temperature: 0.7,
  stream: true
});

let text = '';
for await (const event of stream) {
  if (event.choices[0].finish_reason) {
    break;
  }

  const content = event.choices[0].text;
  text += content;
  console.log(content);
}

console.log({ text });
Basic usage (Closed Source)


javascript

python

http
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "BYTEZ_KEY",
  baseURL: "https://api.bytez.com/models/v2/openai/v1"
});

const response = await client.completions.create({
  model: "openai/davinci-002",
  prompt: "Write a short poem about AI",
  temperature: 0.7,
  max_tokens: 150
});

console.log(response);
Streaming (Closed Source)


javascript

python

http
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "BYTEZ_KEY",
  baseURL: "https://api.bytez.com/models/v2/openai/v1"
});

const stream = await client.completions.create({
  model: "openai/davinci-002",
  prompt: "Write a short poem about AI",
  max_tokens: 150,
  temperature: 0.7,
  stream: true
});

let text = '';
for await (const event of stream) {
  if (event.choices[0].finish_reason) {
    break;
  }

  const content = event.choices[0].text;
  text += content;
  console.log(content);
}

console.log({ text });


Chat Models
Get started
Use the latest AI chat models

Basic usage


javascript

python

http
import Bytez from 'bytez.js';

// insert your key
const sdk = new Bytez('BYTEZ_KEY');

// choose your model
const model = sdk.model('Qwen/Qwen3-4B');

// provide the model with input
const input = [
  {
    "role": "system",
    "content": "You are a friendly chatbot"
  },
  {
    "role": "assistant",
    "content": "Hello, I'm a friendly bot"
  },
  {
    "role": "user",
    "content": "Hello bot, what is the capital of England?"
  }
];

// send to the model
const { error, output } = await model.run(input);

// observe the output
console.log({ error, output });

Add params


javascript

python

http
import Bytez from 'bytez.js';

// insert your key
const sdk = new Bytez('BYTEZ_KEY');

// choose your model
const model = sdk.model('Qwen/Qwen3-4B');

// provide the model with input
const input = [
  {
    "role": "system",
    "content": "You are a friendly chatbot"
  },
  {
    "role": "assistant",
    "content": "Hello, I'm a friendly bot"
  },
  {
    "role": "user",
    "content": "Hello bot, what is the capital of England?"
  }
];

// provide the model with params
const params = {
  "temperature": 0
};

// send to the model
const { error, output } = await model.run(input, params);

// observe the output
console.log({ error, output });

Stream text


javascript

python

http
import Bytez from 'bytez.js';

// insert your key
const sdk = new Bytez('BYTEZ_KEY');

// choose your model
const model = sdk.model('Qwen/Qwen3-4B');

// provide the model with input
const input = [
  {
    "role": "system",
    "content": "You are a friendly chatbot"
  },
  {
    "role": "assistant",
    "content": "Hello, I'm a friendly bot"
  },
  {
    "role": "user",
    "content": "Hello bot, what is the capital of England?"
  }
];

// set streaming to "true"
const stream = true;

// send to the model
const readStream = await model.run(input, stream);

let text = '';

for await (const tokens of readStream) {
  text += tokens;

  console.log(tokens);
}

// observe the output
console.log({ text });

Add params + Stream text


javascript

python

http
import Bytez from 'bytez.js';

// insert your key
const sdk = new Bytez('BYTEZ_KEY');

// choose your model
const model = sdk.model('Qwen/Qwen3-4B');

// provide the model with input
const input = [
  {
    "role": "system",
    "content": "You are a friendly chatbot"
  },
  {
    "role": "assistant",
    "content": "Hello, I'm a friendly bot"
  },
  {
    "role": "user",
    "content": "Hello bot, what is the capital of England?"
  }
];

// provide the model with params
const params = {
  "temperature": 0
};

// set streaming to "true"
const stream = true;

// send to the model
const readStream = await model.run(input, params, stream);

let text = '';

for await (const tokens of readStream) {
  text += tokens;

  console.log(tokens);
}

// observe the output
console.log({ text });


Chat Models
Closed source models
Using chat and chat multi-modal models with closed source providers (OpenAI, Anthropic, etc)


javascript

python

http
import Bytez from "bytez.js";

// insert your key
const sdk = new Bytez("BYTEZ_KEY");

// choose your chat model + insert your provider key
// const model = sdk.model("google/gemini-2.0-flash", "YOUR_GEMINI_KEY");
const model = sdk.model("openai/gpt-4o", "YOUR_OPEN_AI_KEY");

// provide the model your chat session
const messages = [
{ role: "system", content: "You are a friendly chatbot" },
{ role: "assistant", content: "Hello, I'm a friendly bot" },
{ role: "user", content: "Hello bot, what is the capital of England?" },
]
// send to model
const { error, output, provider } = await model.run(messages);

// `provider` is the raw OpenAI output
console.log({ error, output, provider });


Language Models
text2text-generation
Generate text from input text for applications like text completion, content generation, and dialogue systems

Basic usage

Send a prompt to a model to generate text

javascript

python

http
import Bytez from 'bytez.js';

// insert your key
const sdk = new Bytez('BYTEZ_KEY');

// choose your model
const model = sdk.model('google/flan-t5-base');

// provide the model with input
const input = "Once upon a time there was a beautiful home where";

// provide the model with params
const params = {
  "max_new_tokens": 200,
  "min_new_tokens": 50,
  "temperature": 0.5
};

// send to the model
const { error, output } = await model.run(input, params);

// observe the output
console.log({ error, output });

Language Models
text-generation
Generate text from an initial prompt for applications like story generation, dialogue systems, and creative writing

Basic usage

Send a prompt to a model to generate text

javascript

python

http
import Bytez from 'bytez.js';

// insert your key
const sdk = new Bytez('BYTEZ_KEY');

// choose your model
const model = sdk.model('openai-community/gpt2');

// provide the model with input
const input = "Once upon a time there was a beautiful home where";

// send to the model
const { error, output } = await model.run(input);

// observe the output
console.log({ error, output });


Multimodal Models
document-question-answering
Answer questions based on document content for tasks like contract analysis, document understanding, and information retrieval

Basic usage

Send an image of a document along with a question to get relevant answers

javascript

python

http


import Bytez from 'bytez.js';

// insert your key
const sdk = new Bytez('BYTEZ_KEY');

// choose your model
const model = sdk.model('cloudqi/CQI_Visual_Question_Awnser_PT_v0');

// provide the model with input
const input = {
  "question": "Whats the total cost?",
  "url": "https://templates.invoicehome.com/invoice-template-us-neat-750px.png"
};

// send to the model
const { error, output } = await model.run(input);

// observe the output
console.log({ error, output });

Multimodal Models
visual-question-answering
Answer questions based on image content for applications like interactive learning, accessibility features, and content analysis

Basic usage

Send an image URL and a question to receive an answer

javascript

python

http


import Bytez from 'bytez.js';

// insert your key
const sdk = new Bytez('BYTEZ_KEY');

// choose your model
const model = sdk.model('Salesforce/blip-vqa-base');

// provide the model with input
const input = {
  "question": "What kind of animal is this?",
  "image": "https://ocean.si.edu/sites/default/files/styles/3_2_largest/public/2023-11/Screen_Shot_2018-04-16_at_1_42_56_PM.png.webp?itok=Icvi-ek9"
};

// send to the model
const { error, output } = await model.run(input);

// observe the output
console.log({ error, output });


Overview
An Overview of CRUD Operations & Unified Input Schemas for SOTA AI models

​
.